# Apache Spark
เป็นเครื่องมือหนึ่งที่ถูกดีไซน์มาให้เราใช้งานแบบทำงานกลุ่มได้ โดยที่เชื่อมต่อระบบการทำงานของคอมพิวเตอร์เข้าด้วยกัน หรือเรียกว่า Cluster computing platform ซึ่งสามารถกระจายงานที่ต้องทำไปยังเครื่องอื่นๆภายในระบบ ทำให้เราสามารถประมวลผลข้อมูลขนาดใหญ่แบบเต็มประสิทธิภาพ หรือแบบ real-time ไปพร้อม ๆ กันได้ ตัวชูโรงของ Spark คือการใช้คอนเซ็ปต์ RDD

![500](../../_assets/data_engineer/distributed_systems/apache_spark_logo.png)

## Resilient Distributed Datasets (RDD)
คือโครงสร้างข้อมูลพื้นฐานของ Spark เป็นชุดข้อมูลไม่สามารถเขียนทับได้ (immutable) ทนต่อการล่มได้เพราะจะเก็บประวัติการเปลี่ยนแปลงของข้อมูลไว้ในหลายโหนด หากมีโหนดไหนเสียไปก็สามารถเรียกจากอีกโหนดได้

แต่ละ Dataset ใน Spark จะถูกแบ่งเป็น partition ทั่วทั้ง cluster และยังสามารถทำงานแบบขนานกันได้ (parallel) ระหว่าง node ใน cluster โดยข้อมูลเหล่านี้ถูกสร้างขึ้นโดยการกำหนดการทำงานว่า
1. ทำบนข้อมูลบน stable storage
2. ทำด้วย mutable และ immutable collection ที่มีอยู่
3. ทำด้วยไฟล์ภายนอกใน Hadoop HDFS หรือเป็นระบบที่รองรับ
ข้อมูลเหล่านี้จะถูกเก็บบนหน่วยความจำ (Memory) ดังนั้นจึงสามารถเรียกใช้ซ้ำได้อย่างรวดเร็ว และมีประสิทธิภาพ

มี Key feature หลัก ๆ ดังนี้
### Lazy Evaluation
คำสั่งแบบ Transformation ทั้งหมดจะไม่ทำงานเลย (Lazy) แต่จะเก็บเป็นประวัติไว้ก่อน Spark จะประมวลผลเมื่อมีคำสั่งแบบ Action มาสั่งให้ทำ

### In-Memory Computation
ข้อมูลถูกเก็บไว้ใน RAM (random access memory) แทนที่จะเก็บไว้ใน disk ส่งผลให้ใช้พื้นที่จัดเก็บข้อมูลน้อยลง เมื่อทำซ้ำ ๆ จะสามารถจำ pattern ทำให้วิเคราะห์ข้อมูลขนาดใหญ่ได้มีประสิทธิภาพมากขึ้น

### Fault Tolerance

### Immutability
### Partitioning
### Location Setup Capability

## ข้อดีของ Apache Spark
- เก็บข้อมูลบนหน่วยความจำ (Memory) แทนการเขียนไฟล์เก็บข้อมูลทุกขั้นตอน
- เก็บการเปลี่ยนแปลงข้อมูล แทนการเก็บข้อมูลที่เปลี่ยนแล้ว
- ทนต่อการล่ม (Fault-tolerant) ด้วย RDD (Resilient Distributed Dataset)
- รองรับหลายภาษา เช่น Python, Java, Scala, R
- มี Component ให้ใช้งานเยอะ เช่น Spark SQl, MLlib, GraphX







